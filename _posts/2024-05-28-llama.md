---
layout: post
title: LLAMA 설치형 AI
category: AI
tags:
---

# LLAMA 설치형 AI
* 참고 영상 : https://www.youtube.com/watch?v=wUPr5Op3feY
* https://ollama.com/
* download 후 설치
* ```ollama run llama2```


---

# 다른모델 사용하기
* https://ollama.com/library
* 모델 선택 후 실행 명령어 카피하여 터미널에 붙여넣고 엔터


---

# ollama api
* https://github.com/ollama/ollama/blob/main/docs/api.md

---

# llama3
* https://ollama.com/library/llama3
* ```ollama run llama3```

```
curl -X POST http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt":"Why is the sky blue?"
 }'
```

## 모델변경
```
Instruct는 채팅/대화 사용 사례에 맞게 미세 조정되었습니다.
예: ollama run llama3 -> ollama run llama3:70b
Pre-trained 는 기본 모델입니다.
예: ollama run llama3:text -> ollama run llama3:70b-text
```
* ollama run llama3 4.7gb 필요
* ollama run llama3:70b 는 39gb 필요

## 모델 위치
* C:\Users\%username%\.ollama\models

## 모델 위치 변경하기
* 환경변수로 모델 경로 지정
* ``` setx OLLAMA_MODELS 경로명 ```

---

## llama3 한글
* https://devmeta.tistory.com/74?category=947359
* https://devmeta.tistory.com/80

---

## 한글모델
* <https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B>
* <https://huggingface.co/Bllossom/llama-3-Korean-Bllossom-70B>

```
python .\convert.py C:\Users\llama-3-Korean-Bllossom-8B --outfile llama3-ko-16.gguf --outtype f16 --vocab-type bpe --pad-vocab
```

---

### Convert the model
* Note: some model architectures require using specific convert scripts. For example, Qwen models require running convert-hf-to-gguf.py instead of convert.py
* python llm/llama.cpp/convert.py ./model --outtype f16 --outfile converted.bin

### Quantize the model
* llm/llama.cpp/quantize converted.bin quantized.bin q4_0

---

* 허깅페이스 저장소를 다운로드 하기 위해서 cli 설치가 필요하다
* cmd 터미널에서 다음의 명령어를 입력하면 됨

 # CLI(Command Line Interface) 설치
 * ``` pip install -U "huggingface_hub[cli]" ```
  
## Download an entire repository
* ``` huggingface-cli download 저장소 주소 ``` : 캐시에 저장됨
* ``` huggingface-cli download 저장소 주소 --local-dir 디렉토리명 ``` : 로컬 디렉토리에 저장됨

---

# safetensors -> gguf
* 참고 : https://github.com/ollama/ollama/blob/main/docs/import.md

# 레파지토리 복사
```
git clone [git@github.com:ollama/ollama.git](https://github.com/ollama/ollama.git) ollama
cd ollama
```

# 서브모듈 업데이트
```
git submodule init
git submodule update llm/llama.cpp
```

# 파이썬 의존성 설치
```
python3 -m venv llm/llama.cpp/.venv
source llm/llama.cpp/.venv/bin/activate  <- 리눅스 환경에서 입력하는 명령어
ollama\llm\llama.cpp\.venv\Scripts\acvivate <- 윈도우 환경에서는 해당 폴더의 Acvivate.bat 파일을 실행하면 됩니다.
액티베이트를 시킨 이후의 다음의 커멘트 입력시, 경로가 맞지 않다고 하여 절대경로를 입력함
pip install -r llm/llama.cpp/requirements.txt
```

# Quantize 툴 빌드
```
make -C llm/llama.cpp quantize
```
* 일단 에러나면서 실패함 activate 문제인가

---

# 모델 변경
```
python llm/llama.cpp/convert.py ./model --outtype f16 --outfile converted.bin
python llm/llama.cpp/convert.py E:\hugging_face\llama-3-korean-bllossom-8b  --outtype f16 --outfile converted.bin
토크나이져를 못찾았다는에러
--vocab-type bpe --pad-vocab 옵션 추가
python llm/llama.cpp/convert.py E:\hugging_face\llama-3-korean-bllossom-8b  --outtype f16 --outfile converted.bin --vocab-type bpe --pad-vocab
python llm/llama.cpp/convert.py E:\hugging_face\llama-3-korean-bllossom-70b  --outtype f16 --outfile converted.bin --vocab-type bpe --pad-vocab
```
* 위 명령어 대로라면 llm 폴더가 있는 곳에 converted.bin 파일로 만들어짐 llama3-ko-16.gguf 식으로 이름바꾸기

## 모델을 위한 모드파일(Modefile)작성
* 이름.modefile

```
FROM 파일이름입력

TEMPLATE """{{- if .System }}
<s>{{ .System }}</s>
{{- end }}
<s>Human:
{{ .Prompt }}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. 모든 대답은 한국어(Korean)으로 대답해줘."""

PARAMETER temperature 0
PARAMETER num_predict 3000
PARAMETER num_ctx 4096
PARAMETER stop <s>
PARAMETER stop </s>
```
* Modefile과 guff파일을 동일 경로에 위치
* powershell에서 파일이 있는 경로로 이동 후 다음의 명령어를 실행
```ollama create 새모델이름 -f 모드파일이름```

---
---

# WebUI
* 참고 : https://devmeta.tistory.com/72

## docker 설치하기
* https://docs.docker.com/desktop/install/windows-install/
  
## window powershell 실행하기
* 시작메뉴에서 검색한 후 관리자 권한으로 실행
```
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

* 다운로드 및 실행이 진행된다.

* 웹 브라우저에서 : http://localhost:3000/ 접속